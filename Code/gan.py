# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ct6tUXa7X9yPLlOFE-LxI8ZqCaxhlQu_
"""

import time
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Input, Embedding, Flatten, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from google.colab import drive
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

# ---- 1. Mount Google Drive ----
drive.mount('/content/drive')

# ---- 2. Define dataset paths ----
train_path = "/content/drive/My Drive/Dataset/Train_set.csv"
test_path = "/content/drive/My Drive/Dataset/Test_set.csv"
val_path = "/content/drive/My Drive/Dataset/Validation_set.csv"

# ---- 3. Load datasets ----
train_df = pd.read_csv(train_path)
test_df = pd.read_csv(test_path)
val_df = pd.read_csv(val_path)

# ---- 4. Separate features and labels ----
X_train, y_train = train_df.drop(columns=["type"]).values, train_df["type"].values
X_test, y_test = test_df.drop(columns=["type"]).values, test_df["type"].values
X_val, y_val = val_df.drop(columns=["type"]).values, val_df["type"].values

# ---- 5. Normalize between -1 and 1 ----
scaler = MinMaxScaler(feature_range=(-1, 1))
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_val = scaler.transform(X_val)

input_dim = X_train.shape[1]
latent_dim = 100

# ---- 6. Build Conditional Generator ----
def build_generator():
    noise_input = Input(shape=(latent_dim,))
    label_input = Input(shape=(1,), dtype='int32')

    label_embedding = Embedding(input_dim=2, output_dim=50)(label_input)
    label_embedding = Flatten()(label_embedding)

    model_input = Concatenate()([noise_input, label_embedding])

    x = Dense(256)(model_input)
    x = LeakyReLU(alpha=0.2)(x)
    x = BatchNormalization(momentum=0.8)(x)

    x = Dense(512)(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = BatchNormalization(momentum=0.8)(x)

    output = Dense(input_dim, activation='tanh')(x)

    model = Model([noise_input, label_input], output)
    return model

# ---- 7. Build Conditional Discriminator ----
def build_discriminator():
    feature_input = Input(shape=(input_dim,))
    label_input = Input(shape=(1,), dtype='int32')

    label_embedding = Embedding(input_dim=2, output_dim=50)(label_input)
    label_embedding = Flatten()(label_embedding)

    model_input = Concatenate()([feature_input, label_embedding])

    x = Dense(512)(model_input)
    x = LeakyReLU(alpha=0.2)(x)

    x = Dense(256)(x)
    x = LeakyReLU(alpha=0.2)(x)

    output = Dense(1, activation='sigmoid')(x)

    model = Model([feature_input, label_input], output)
    model.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# ---- 8. Initialize Models ----
generator = build_generator()
discriminator = build_discriminator()
discriminator.trainable = False

noise_input = Input(shape=(latent_dim,))
label_input = Input(shape=(1,), dtype='int32')
generated_features = generator([noise_input, label_input])
validity = discriminator([generated_features, label_input])
combined = Model([noise_input, label_input], validity)
combined.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')

# ---- 9. Train the cGAN ----
def train_gan(epochs=3000, batch_size=256):
    d_losses, g_losses = [], []
    train_start_time = time.time()

    for epoch in range(epochs):
        idx = np.random.randint(0, X_train.shape[0], batch_size)
        real_data = X_train[idx]
        real_labels = y_train[idx].reshape(-1, 1)

        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        gen_labels = np.random.randint(0, 2, (batch_size, 1))  # both 0 and 1
        gen_data = generator.predict([noise, gen_labels])

        y_real = np.ones((batch_size, 1)) * 0.9
        y_fake = np.zeros((batch_size, 1)) + 0.1

        d_loss_real = discriminator.train_on_batch([real_data, real_labels], y_real)
        d_loss_fake = discriminator.train_on_batch([gen_data, gen_labels], y_fake)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
        d_losses.append(d_loss[0])

        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        attack_labels = np.ones((batch_size, 1))  # Generator learns to create attacks
        g_loss = combined.train_on_batch([noise, attack_labels], np.ones((batch_size, 1)))
        g_losses.append(g_loss)

        if epoch % 500 == 0:
            print(f"Epoch {epoch} - D Loss: {d_loss[0]:.4f}, G Loss: {g_loss:.4f}")
            save_generated_samples(epoch)

    train_end_time = time.time()
    print(f"\nTraining completed in {train_end_time - train_start_time:.2f} seconds")

    plt.figure(figsize=(10, 5))
    plt.plot(d_losses, label='Discriminator Loss')
    plt.plot(g_losses, label='Generator Loss')
    plt.title('GAN Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# ---- 10. Save Sample Attacks ----
def save_generated_samples(epoch):
    noise = np.random.normal(0, 1, (5, latent_dim))
    labels = np.ones((5, 1))  # Generate attack samples
    samples = generator.predict([noise, labels])
    print(f"Generated Sample (Epoch {epoch}):", samples[:2])

# ---- 11. Test GAN ----
def test_gan(batch_size=512):
    test_start_time = time.time()

    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    test_labels = np.random.randint(0, 2, size=(batch_size, 1))
    generated_data = generator.predict([noise, test_labels])

    idx = np.random.randint(0, X_test.shape[0], batch_size)
    real_data = X_test[idx]
    real_labels = y_test[idx].reshape(-1, 1)

    X_test_combined = np.vstack((real_data, generated_data))
    y_test_combined = np.vstack((np.ones((batch_size, 1)) * 0.9, np.zeros((batch_size, 1)) + 0.1))
    labels_combined = np.vstack((real_labels, test_labels))

    test_loss, test_accuracy = discriminator.evaluate([X_test_combined, labels_combined], y_test_combined)
    print(f"\nDiscriminator Test Accuracy: {test_accuracy * 100:.2f}%")

    y_pred = (discriminator.predict([X_test_combined, labels_combined]) > 0.5).astype(int)
    y_true = (y_test_combined > 0.5).astype(int)

    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

    print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}")
    print(f"FAR: {fp / (fp + tn):.4f}, MDR: {fn / (fn + tp):.4f}")
    print(f"GAN Testing Time: {time.time() - test_start_time:.2f} seconds")

# ---- 12. Generate Final Synthetic Attack Data ----
def generate_attack_samples(n_samples=10000):
    noise = np.random.normal(0, 1, (n_samples, latent_dim))
    attack_labels = np.ones((n_samples, 1))
    generated_attacks = generator.predict([noise, attack_labels])
    original_scale = scaler.inverse_transform(generated_attacks)

    synthetic_df = pd.DataFrame(original_scale, columns=train_df.columns[:-1])
    synthetic_df["type"] = 1  # Label as attack
    synthetic_df.to_csv("/content/drive/My Drive/Dataset/Synthetic_Attacks_cGAN.csv", index=False)
    print(f"\nSaved {n_samples} synthetic attack samples.")

# ---- 13. Train and Test ----
train_gan(epochs=300, batch_size=256)
test_gan()
generate_attack_samples(n_samples=10000)