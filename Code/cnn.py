# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nelJgRn_klp-as2RdlcQ8EstXR2I9q-W
"""

import time
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler
from google.colab import drive
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix

# ---- 1. Mount Google Drive ----
drive.mount('/content/drive')

# ---- 2. Define dataset paths in Google Drive ----
train_path = "/content/drive/My Drive/Dataset/Train_set.csv"
test_path = "/content/drive/My Drive/Dataset/Test_set.csv"
val_path = "/content/drive/My Drive/Dataset/Validation_set.csv"

# ---- 3. Load datasets ----
train_df = pd.read_csv(train_path)
test_df = pd.read_csv(test_path)
val_df = pd.read_csv(val_path)

# ---- 4. Separate features and target ----
X_train, y_train = train_df.drop(columns=["type"]).values, train_df["type"].values
X_test, y_test = test_df.drop(columns=["type"]).values, test_df["type"].values
X_val, y_val = val_df.drop(columns=["type"]).values, val_df["type"].values

# ---- 5. Normalize features ----
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_val = scaler.transform(X_val)

# ---- 6. Reshape data for CNN input (samples, timesteps, features) ----
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)

# ---- 7. Build CNN Model ----
cnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),
    BatchNormalization(),
    Dropout(0.3),
    Conv1D(filters=128, kernel_size=3, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Binary classification
])

# Compile the model
cnn_model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

# ---- 8. Measure training time ----
train_start_time = time.time()

history_cnn = cnn_model.fit(X_train, y_train,
                            epochs=5,
                            batch_size=32,
                            validation_data=(X_val, y_val))

train_end_time = time.time()
print(f"\nCNN Training Time: {train_end_time - train_start_time:.2f} seconds")

# ---- 9. Evaluate the Model ----
test_start_time = time.time()

loss, accuracy = cnn_model.evaluate(X_test, y_test)

test_end_time = time.time()
print(f"CNN Testing Time: {test_end_time - test_start_time:.2f} seconds")

print(f"CNN Test Accuracy: {accuracy * 100:.2f}%")

# ---- 10. Compute Precision, Recall, F1-Score ----
# Generate predictions
y_pred_prob = cnn_model.predict(X_test)
y_pred_labels = (y_pred_prob > 0.5).astype(int).flatten()
y_test = y_test.astype(int)

precision = precision_score(y_test, y_pred_labels)
recall = recall_score(y_test, y_pred_labels)
f1 = f1_score(y_test, y_pred_labels)

print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# Confusion matrix
tn, fp, fn, tp = confusion_matrix(y_test, y_pred_labels).ravel()

false_alarm_rate = fp / (fp + tn)
missed_detection_rate = fn / (fn + tp)

print(f"False Alarm Rate (FAR): {false_alarm_rate:.4f}")
print(f"Missed Detection Rate (MDR): {missed_detection_rate:.4f}")

# ---- 11. Plot CNN Training History ----
plt.figure(figsize=(10, 5))
plt.plot(history_cnn.history['accuracy'], label='Train Accuracy')
plt.plot(history_cnn.history['val_accuracy'], label='Validation Accuracy')
plt.title('CNN Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()