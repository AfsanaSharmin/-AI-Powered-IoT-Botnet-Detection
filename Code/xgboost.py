# -*- coding: utf-8 -*-
"""xgboost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xRkmBNbl84bcVpz2xrXLwMqVFpANCE6o
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
#rom sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, accuracy_score

df_oversampled = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/ML Project/new_resampled.csv")

test_size = 0.3

# Identify the indices for each class
type_0 = df_oversampled[df_oversampled['type'] == 0].index
type_1 = df_oversampled[df_oversampled['type'] == 1].index

# Calculate the number of samples for testing for each class
count_0 = int(len(type_0) * test_size)
count_1 = int(len(type_1) * test_size)

# Fix the indices for the test set for each class
test_0 = type_0[:count_0]
test_1 = type_1[:count_1]

test_set = pd.Index(np.concatenate([test_0, test_1]))

# Calculate the number of samples for testing
num_test_samples = int(len(df_oversampled) * test_size)

# Get the training indices
train_set = df_oversampled.index.difference(test_set)

# Split the data into a training set and a fixed test set
X_train = df_oversampled.drop(columns=['type']).loc[train_set]
y_train = df_oversampled['type'].loc[train_set]
X_test = df_oversampled.drop(columns=['type']).loc[test_set]
y_test = df_oversampled['type'].loc[test_set]

#df_oversampled

# Define a parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [5,20],
    'max_depth': [2,4],
    'learning_rate': [0.5, 0.7],
}

# Initialize the XGBoost classifier
xgb_model = XGBClassifier()

# Perform cross-validation with hyperparameter tuning
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=10)
grid_search.fit(X_train, y_train)

# Get the best parameters from cross-validation
best_params = grid_search.best_params_
best_xgb = XGBClassifier(**best_params)
best_xgb.fit(X_train, y_train)

y_pred = best_xgb.predict(X_test)

# XGBoost model (best_xgb) during the hyperparameter tuning phase

# Print the best hyperparameter values
print("Best Hyperparameters:")
print(f"n_estimators: {best_params['n_estimators']}")
print(f"max_depth: {best_params['max_depth']}")
print(f"learning_rate: {best_params['learning_rate']}")


# Make predictions on the test data
y_pred = best_xgb.predict(X_test)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)


import matplotlib.pyplot as plt

# Visualize feature importance
xgb.plot_importance(best_xgb)
plt.show()