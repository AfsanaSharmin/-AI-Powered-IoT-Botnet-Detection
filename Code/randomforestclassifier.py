# -*- coding: utf-8 -*-
"""RandomForestClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MAoC_YGJAZibBIDyJDpens91HmbmafJz
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

df_oversampled = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/ML Project/new_resampled.csv")

test_size = 0.3

# Identify the indices for each class
type_0 = df_oversampled[df_oversampled['type'] == 0].index
type_1 = df_oversampled[df_oversampled['type'] == 1].index

# Calculate the number of samples for testing for each class
count_0 = int(len(type_0) * test_size)
count_1 = int(len(type_1) * test_size)

# Fix the indices for the test set for each class
test_0 = type_0[:count_0]
test_1 = type_1[:count_1]

test_set = pd.Index(np.concatenate([test_0, test_1]))

# Calculate the number of samples for testing
num_test_samples = int(len(df_oversampled) * test_size)

# Get the training indices
train_set = df_oversampled.index.difference(test_set)

# Split the data into a training set and a fixed test set
X_train = df_oversampled.drop(columns=['type']).loc[train_set]
y_train = df_oversampled['type'].loc[train_set]
X_test = df_oversampled.drop(columns=['type']).loc[test_set]
y_test = df_oversampled['type'].loc[test_set]

# Define a parameter grid to search through for Random Forest
param_grid_rf = {
    'n_estimators': [5,10,15],  # Number of trees in the forest
    'max_depth': [2, 5,7],
    'min_samples_split': [2, 5,7],
    'min_samples_leaf': [2,4,6],
}

# Initialize the Random Forest classifier
rf = RandomForestClassifier()

# Perform cross-validation with hyperparameter tuning for Random Forest
grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=10)
grid_search_rf.fit(X_train, y_train)

# Get the best parameters from cross-validation
best_params_rf = grid_search_rf.best_params_

best_rf = RandomForestClassifier(**best_params_rf)
best_rf.fit(X_train, y_train)


# Get the optimal hyperparameters from the best_rf model
n_estimators = best_rf.n_estimators
max_depth_rf = best_rf.max_depth
min_samples_split_rf = best_rf.min_samples_split
min_samples_leaf_rf = best_rf.min_samples_leaf

# Print the best hyperparameter values
print("Best Hyperparameters:")
print(f"n_estimators: {best_params_rf['n_estimators']}")
print(f"max_depth: {best_params_rf['max_depth']}")
print(f"min_samples_split: {best_params_rf['min_samples_split']}")
print(f"min_samples_leaf: {best_params_rf['min_samples_leaf']}")

# Make predictions on the test data using the Random Forest model
y_pred_rf = best_rf.predict(X_test)

# Calculate the confusion matrix for Random Forest
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)
print("Confusion Matrix (Random Forest):")
print(conf_matrix_rf)

# Calculate the accuracy of the Random Forest model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print("Accuracy (Random Forest):", accuracy_rf)

from sklearn import tree
import matplotlib.pyplot as plt

# Accessing the first tree in the Random Forest (assuming it exists)
first_tree = best_rf.estimators_[0]

# Plotting the first tree
plt.figure(figsize=(14, 10))
tree.plot_tree(first_tree, feature_names=X_train.columns, class_names=["benign", "attack"], filled=True, fontsize=12)
plt.show()