# -*- coding: utf-8 -*-
"""decisionTree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1crD_jshE0oL71Dnfe-K0NmIFbUpG0Lfk
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler

df_oversampled = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/ML Project/new_resampled.csv")

test_size = 0.3

# Identify the indices for each class
type_0 = df_oversampled[df_oversampled['type'] == 0].index
type_1 = df_oversampled[df_oversampled['type'] == 1].index

# Calculate the number of samples for testing for each class
count_0 = int(len(type_0) * test_size)
count_1 = int(len(type_1) * test_size)

# Fix the indices for the test set for each class
test_0 = type_0[:count_0]
test_1 = type_1[:count_1]

test_set = pd.Index(np.concatenate([test_0, test_1]))

# Calculate the number of samples for testing
num_test_samples = int(len(df_oversampled) * test_size)

# Get the training indices
train_set = df_oversampled.index.difference(test_set)

# Split the data into a training set and a fixed test set
X_train = df_oversampled.drop(columns=['type']).loc[train_set]
y_train = df_oversampled['type'].loc[train_set]
X_test = df_oversampled.drop(columns=['type']).loc[test_set]
y_test = df_oversampled['type'].loc[test_set]

len(df_oversampled)

"""Decision Tree"""

# Define a parameter grid to search through
param_grid = {
    'max_depth': [2, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [2, 6, 10],
}

# Initialize the Decision Tree classifier
dt = DecisionTreeClassifier()

# Perform cross-validation with hyperparameter tuning
grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=10)
grid_search.fit(X_train, y_train)

# Get the best parameters from cross-validation
best_params = grid_search.best_params_

best_dt = DecisionTreeClassifier(**best_params)
best_dt.fit(X_train, y_train)

#Decision Tree model (best_dt) during the hyperparameter tuning phase

# Print the best hyperparameter values
print("Best Hyperparameters:")
print(f"n_estimators: {best_params['n_estimators']}")
print(f"max_depth: {best_params['max_depth']}")
print(f"min_samples_split: {best_params['min_samples_split']}")
print(f"min_samples_leaf: {best_params['min_samples_leaf']}")


# Get the optimal hyperparameters from the best_dt model
max_depth = best_dt.tree_.max_depth
min_samples_split = best_dt.min_samples_split
min_samples_leaf = best_dt.min_samples_leaf

# Calculate the number of nodes in the tree
total_nodes = 2 ** (max_depth + 1) - 1

# Calculate the number of leaf nodes (terminal nodes)
n_leaves = best_dt.get_n_leaves()

print(f"Total nodes in the tree: {total_nodes}")
print(f"Number of leaf nodes (terminal nodes): {n_leaves}")

import matplotlib.pyplot as plt

# Values
#total_nodes = 63
#n_leaves = 6


from sklearn.metrics import confusion_matrix, accuracy_score

# Assuming you have a trained Decision Tree model (best_dt)
# If you don't have the model, you can train it using the previous code

# Make predictions on the test data
y_pred = best_dt.predict(X_test)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Bar chart
fig, ax = plt.subplots()
x = ['Total Nodes', 'Leaf Nodes']
y = [total_nodes, n_leaves]
bars = ax.bar(x, y, color=['blue', 'green'])

ax.set_ylabel('Count',fontsize=18, fontweight='bold')
#ax.set_title('Total Nodes vs Leaf Nodes')

# Setting font size for x-axis labels
for tick in ax.get_xticklabels():
    tick.set_fontsize(18)

# Setting font size for y-axis tick labels
ax.tick_params(axis='y', labelsize=16)

plt.show()

import matplotlib.pyplot as plt

# Assuming you have a trained Decision Tree model (best_dt in this case)
# If you don't have the model, you can train it using the previous code

# Visualize the Decision Tree
plt.figure(figsize=(8, 8))
plot_tree(best_dt, feature_names=X_train.columns, class_names=["benign", "attack"], filled=True)
plt.show()